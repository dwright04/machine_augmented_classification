{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Machine Augmented Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The basic idea is to take advantage of underlying structure in an initially unlabelled data set.  Subjects are grouped into similar clusters based on proximity in feature space.\n",
    "\n",
    "Volunteers can then decide what meaning each cluster has or if a cluster should be \"dissolved\".\n",
    "\n",
    "Volunteers are provided with a list of predefined labels they can apply either to an entire cluster (assigning a meaning to the custer), or to individual subjects within a cluster (\"dissolving\" a cluster that groups subjects belonging to multiple classes).\n",
    "\n",
    "After a pass over the data volunteers will have assigned labels to the data set.  An expert can review cluster labels and decide whether to merge or dissolve clusters based on domain knowledge.\n",
    "\n",
    "A machine can be trained based on these labels.  The aim of this machine is to transform the data into a new feature space such that subjects from dissolved clusters now lie in distinct regions of the new feature space based on the new labels.  Well defined clusters may become even more tightly clustered in the new space.\n",
    "\n",
    "Performance tracking can still be used here as volunteers label gold standard clusters or subsets of clusters or artificially contaminated clusters.\n",
    "\n",
    "The labels that the machine is learning need not be exactly what the research team are looking for, but they can assign their own meaning on top of the volunteer labels.\n",
    "\n",
    "Data sets that machines and humans are good at dealing with should fall out naturally.  A machine will naturally find it easy to classify classes that lie far from each other in feature space.  Humans will be good at identifying the \"odd ones out\" in clusters that confuse classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Jeremy Howard [TED talk](https://youtu.be/t4kyRyKyOpo?t=729) that captures the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import plotly\n",
    "import numpy as np\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plotly.tools.set_credentials_file(username=os.environ[\"PLOTLY_USERNAME\"], api_key=os.environ[\"PLOTLY_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define a function for interactive data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def threeDPlot(X, indices, not_indices, label, not_label):\n",
    "  trace1 = go.Scatter3d(\n",
    "    x=X[indices,0],\n",
    "    y=X[indices,1],\n",
    "    z=X[indices,2],\n",
    "    name=label,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "      size=5,\n",
    "      color='#1E2EDE',\n",
    "      line=dict(\n",
    "        color='rgb(204, 204, 204)',\n",
    "        width=0.1\n",
    "      ),\n",
    "      opacity=0.8\n",
    "    )\n",
    "  )\n",
    "\n",
    "  trace2 = go.Scatter3d(\n",
    "    x=X[not_indices,0],\n",
    "    y=X[not_indices,1],\n",
    "    z=X[not_indices,2],\n",
    "    name=not_label,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "      color='#F5B841',\n",
    "      size=5,\n",
    "      symbol='circle',\n",
    "      line=dict(\n",
    "        color='rgb(204, 204, 204)',\n",
    "        width=0.1\n",
    "      ),\n",
    "      opacity=0.8\n",
    "    )\n",
    "  )\n",
    "\n",
    "  data = [trace1, trace2]\n",
    "\n",
    "  layout = go.Layout(\n",
    "    margin=dict(\n",
    "      l=0,\n",
    "      r=0,\n",
    "      b=0,\n",
    "      t=0\n",
    "    )\n",
    "  )\n",
    "    \n",
    "  fig = go.Figure(data=data, layout=layout)\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## mnist data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets take the mnist data set as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    " \n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# flatten the images for PCA\n",
    "x_train_flattened = np.reshape(x_train, (x_train.shape[0], x_train.shape[1]*x_train.shape[2]))\n",
    "\n",
    "# limit the number of examples to 10000 so we can work with plotly interactive plots\n",
    "x_train = x_train[:10000]\n",
    "x_train_flattened = x_train_flattened[:10000]\n",
    "y_train = y_train[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "User PCA to project this data into 3 dimensions so we can visualise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "x_train_pca = pca.fit_transform(x_train_flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualise the data singling out images labelled zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~SNe/37.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = threeDPlot(x_train_pca, np.where(y_train==0)[0], np.where(y_train!=0)[0], '0', 'not 0')\n",
    "py.iplot(fig, filename='mnist_pca3_label0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use heirarchical clustering (which is unsupervised) to group the subjects based on euclidean distance in the original 784 dimensional pixel space.  Choose 100 clusters for no particular reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n",
       "            connectivity=None, linkage='ward', memory=None, n_clusters=100,\n",
       "            pooling_func=<function mean at 0x110915950>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters = 100\n",
    "clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "clustering.fit(x_train_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-92e24e0c7cfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mx_labelled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mone_hot_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "del x_labelled\n",
    "\n",
    "one_hot_encoded = np_utils.to_categorical(y_train, 10)\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "  cluster_indices = np.where(clustering.labels_ == cluster)[0]\n",
    "  n_assigned_examples = cluster_indices.shape[0]\n",
    "  cluster_labels = one_hot_encoded[cluster_indices]\n",
    "  cluster_label_fractions = np.mean(cluster_labels, axis=0)\n",
    "  dominant_cluster_class = np.argmax(cluster_label_fractions)\n",
    "  print(cluster, n_assigned_examples, dominant_cluster_class, cluster_label_fractions[dominant_cluster_class])\n",
    "  # assign labels based on >= 90% class membership, mimicing human labelling\n",
    "  # I'm assuming that if a cluster is diminated by a singel class volunteers \n",
    "  # will assign that class to it.\n",
    "  if cluster_label_fractions[dominant_cluster_class] >= 0.9:\n",
    "    x = x_train[cluster_indices]\n",
    "    l = np.zeros((x.shape[0], 10))\n",
    "    l[:,dominant_cluster_class] += 1\n",
    "    try:\n",
    "      x_labelled = np.concatenate((x_labelled, x))\n",
    "      labels = np.concatenate((labels, l))\n",
    "      labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "    except NameError:\n",
    "      x_labelled = x\n",
    "      labels = l\n",
    "      labelled_indices = cluster_indices\n",
    "        \n",
    "print(x_labelled.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "m = x_labelled.shape[0]\n",
    "order = np.random.permutation(m)\n",
    "x_labelled = x_labelled[order]\n",
    "x_labelled = x_labelled[:,:,:,np.newaxis]\n",
    "labels = labels[order]\n",
    "\n",
    "unlabelled_indices = np.array([x for x in range(x_train.shape[0]) if x not in labelled_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The cluster id, number of examples assigned to each cluster, the dominant cluster class and the proportion of the cluster belonging to the dominant cluster class are printed out.\n",
    "\n",
    "To replicate volunteer labelling of each class, if the dominant cluster class makes up more than 90% of the cluster then assign the dominant cluster label to that cluster.  This is the same as assuming that if 90% of the data is of one class a volunteer will assign it the label corresponding to 90% of the data in the cluster.\n",
    "\n",
    "This gives us a data set of 6943 labelled examples.  There is an upper limit on the label contamination in this data set of 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define some functions to visualise the data assigned to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getDimensions(n):\n",
    "  dim = int(np.ceil(np.sqrt(n)))\n",
    "  return (dim, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plotCluster(cluster_labels, cluster, X, image_dim, limit=200, cmap='gray_r'):\n",
    "  indices = np.where(cluster_labels == cluster)[0] # get the examples assigned to cluster 0\n",
    "\n",
    "  n = np.where(cluster_labels == cluster)[0].shape[0]\n",
    "  print(n)\n",
    "  if n > limit:\n",
    "    indices = indices[:limit]\n",
    "    n = limit\n",
    "    \n",
    "  dims = getDimensions(n)\n",
    "    \n",
    "  fig = plt.figure(figsize=(20,20))\n",
    "  for i in range(n):\n",
    "    ax = fig.add_subplot(dims[0],dims[1],i+1)\n",
    "    ax.imshow(np.reshape(X[indices[i]], (image_dim,image_dim), order='C'), cmap=cmap)\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualise cluster 0 containing 160 subjects with 98% labelled 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plotCluster(clustering.labels_, 0, x_train_flattened, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualise cluster 13 containing 121 subjects with 98% labelled 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plotCluster(clustering.labels_, 13, x_train_flattened, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualise cluster 38 containing 74 subjects with 55% labelled 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plotCluster(clustering.labels_, 38, x_train_flattened, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualise cluster 2 containing 90 subjects with 48% labelled 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plotCluster(clustering.labels_, 2, x_train_flattened, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train a machine to classify the labelled data set.\n",
    "\n",
    "Using a CNN here, but other architectures might be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calculateAccuracy(model, x, y, n_classes):\n",
    "  preds = model.predict(x)\n",
    "  return 100*np.sum(np.argmax(preds, axis=1)== \\\n",
    "                    np.argmax(np_utils.to_categorical(y, n_classes), axis=1))/ \\\n",
    "          len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build the CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='valid', \\\n",
    "                   activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(GlobalAveragePooling2D('channels_last'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fit the model to the data\n",
    "model.fit(x_labelled, labels, epochs=20, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# determine if test set classes are balanced\n",
    "print(np.sum(np_utils.to_categorical(y_test, 10), axis=0)/np.sum(np_utils.to_categorical(y_test, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Calculate the accuracy of this classifier on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_accuracy = calculateAccuracy(model, x_test[:,:,:,np.newaxis], y_test, 10)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Determine the effect of running this classifier for double the number of epochs.  This is important for later as we want to ensure that any future imporvements to this model are not just because we add epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clone the above model and load its weights after 20 epochs\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(filters=16, kernel_size=2, padding='valid', \\\n",
    "                    activation='relu', input_shape=(28,28,1), \\\n",
    "                    weights=model.layers[0].get_weights())) # load the learned weights from the previous model\n",
    "model2.add(MaxPooling2D(pool_size=2))\n",
    "model2.add(Conv2D(filters=32, kernel_size=2, padding='valid', \\\n",
    "                    activation='relu',weights=model.layers[2].get_weights()))\n",
    "model2.add(MaxPooling2D(pool_size=2))\n",
    "model2.add(Conv2D(filters=64, kernel_size=2, padding='valid', \\\n",
    "                    activation='relu',weights=model.layers[4].get_weights()))\n",
    "model2.add(MaxPooling2D(pool_size=2))\n",
    "model2.add(GlobalAveragePooling2D('channels_last'))\n",
    "model2.add(Dense(10, activation='softmax', weights=model.layers[7].get_weights()))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train the cloned mode for an additional 20 epochs\n",
    "model2.fit(x_labelled, labels, epochs=20, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_accuracy = calculateAccuracy(model2, x_test[:,:,:,np.newaxis], y_test, 10)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Make a clone of the original model striping of the output layer so we can project data into the feature space learned by the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# make a clone of the model above stripping off the output layer and loading the \n",
    "# trained weights after 20 epochs\n",
    "model3 = Sequential()\n",
    "model3.add(Conv2D(filters=16, kernel_size=2, padding='valid', \\\n",
    "                    activation='relu', input_shape=(28,28,1), \\\n",
    "                    weights=model.layers[0].get_weights())) # load the learned weights from the previous model\n",
    "model3.add(MaxPooling2D(pool_size=2))\n",
    "model3.add(Conv2D(filters=32, kernel_size=2, padding='valid', \\\n",
    "                    activation='relu',weights=model.layers[2].get_weights()))\n",
    "model3.add(MaxPooling2D(pool_size=2))\n",
    "model3.add(Conv2D(filters=64, kernel_size=2, padding='valid', \\\n",
    "                    activation='relu',weights=model.layers[4].get_weights()))\n",
    "model3.add(MaxPooling2D(pool_size=2))\n",
    "model3.add(GlobalAveragePooling2D('channels_last'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Project the entire data set (10000 subjects) into the CNN feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "activations = model3.predict(x_train[:,:,:,np.newaxis]) # encode the images\n",
    "print(activations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Reduce this feature representation to 3 dimensions for visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "activations_pca = pca.fit_transform(activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Plot the original data set transformed into the new feature space, distinguishing subjects that were labelled form those that remain unlabelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = threeDPlot(activations_pca, labelled_indices, unlabelled_indices, 'labelled', 'unlabelled')\n",
    "py.iplot(fig, filename='mnist_activations_pca3_labelled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now perform the clustering again in the new feature space again arbitrarily looking for 100 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clustering_activations = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "clustering_activations.fit(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for cluster in range(n_clusters):\n",
    "  cluster_indices = np.where(clustering_activations.labels_ == cluster)[0]\n",
    "  n_assigned_examples = cluster_indices.shape[0]\n",
    "  cluster_labels = one_hot_encoded[cluster_indices]\n",
    "  cluster_label_fractions = np.mean(cluster_labels, axis=0)\n",
    "  dominant_cluster_class = np.argmax(cluster_label_fractions)\n",
    "  print(cluster, n_assigned_examples, dominant_cluster_class, cluster_label_fractions[dominant_cluster_class])\n",
    "  # assign labels based on >= 90% class membership, mimicing human labelling\n",
    "  if cluster_label_fractions[dominant_cluster_class] >= 0.9:\n",
    "    a = activations[cluster_indices]\n",
    "    l = np.zeros((a.shape[0], 10))\n",
    "    l[:,dominant_cluster_class] += 1\n",
    "    try:\n",
    "      a_labelled = np.concatenate((a_labelled, a))\n",
    "      labels = np.concatenate((labels, l))\n",
    "      labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "    except NameError:\n",
    "      a_labelled = a\n",
    "      labels = l\n",
    "      labelled_indices = cluster_indices\n",
    "        \n",
    "print(a_labelled.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "m = a_labelled.shape[0]\n",
    "order = np.random.permutation(m)\n",
    "a_labelled = a_labelled[order]\n",
    "labels = labels[order]\n",
    "\n",
    "unlabelled_indices = np.array([x for x in range(x_train.shape[0]) if x not in labelled_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The same as before the clusters with great than 90% dominant class membership are assinged the label of the dominant class.  The previous set of labels have been forgotten, but a mechanism to take advantage those might help.  This time we get a labelled training set with 4974 subjects.\n",
    "\n",
    "Visualise some of these clusters.\n",
    "\n",
    "Cluster 0 with 151 subjects and 99% labelled 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotCluster(clustering_activations.labels_, 0, x_train, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Cluster 36 with 100 subject 29% labelled 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plotCluster(clustering_activations.labels_, 36, x_train, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(500, activation='relu', input_shape=(activations.shape[1],)))\n",
    "model4.add(Dense(10, activation='softmax'))\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fit the model to the data\n",
    "model4.fit(a_labelled, labels, epochs=20, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_activations = model3.predict(x_test[:,:,:,np.newaxis])\n",
    "test_accuracy = calculateAccuracy(model4, test_activations, y_test, 10)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is an improvement on the 62% achieved above. This suggests that the data has been transformed into a more discriminant feature space.  Although model4 is trained on ~2000 fewer subjects than model2 it is 18% more accurate.\n",
    "\n",
    "Confused clusters such as cluster 36 visualised above would be an example of a cluster to dissolve.  Volunteers would be asked to assign a label to each subject in the cluster if the cluster did not appear to capture anything sinificant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3Pi image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets try this with the PS1 3pi data set.\n",
    "\n",
    "This is more difficult.  The data is labelled into two classes but there is more underlying structure such as different artefact types and signal-to-noise.  The classes are skewed to 3 times more bogus than real, MNIST classes are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/dwright/dev/zoo/data/'\n",
    "file = '3pi_20x20_skew2_signPreserveNorm.mat'\n",
    "data = sio.loadmat(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train = data['X'] # load the pixel data\n",
    "y_train = np.squeeze(data['y']) # load the targets\n",
    "x_test  = data['testX'] # load the pixel data\n",
    "y_test  = np.squeeze(data['testy']) # load the targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Cluster subjects into 20 groups using heirarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_clusters = 20\n",
    "clustering_threepi = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "clustering_threepi.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del x_labelled\n",
    "one_hot_encoded = np_utils.to_categorical(y_train, 2)\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "  cluster_indices = np.where(clustering_threepi.labels_ == cluster)[0]\n",
    "  n_assigned_examples = cluster_indices.shape[0]\n",
    "  cluster_labels = one_hot_encoded[cluster_indices]\n",
    "  cluster_label_fractions = np.mean(cluster_labels, axis=0)\n",
    "  dominant_cluster_class = np.argmax(cluster_label_fractions)\n",
    "  print(cluster, n_assigned_examples, dominant_cluster_class, cluster_label_fractions[dominant_cluster_class])\n",
    "  # assign labels based on >= 95% class membership, mimicing human labelling\n",
    "  # I'm assuming that if a cluster is diminated by a singel class volunteers \n",
    "  # will assign that class to it.\n",
    "  if cluster_label_fractions[dominant_cluster_class] >= 0.95:\n",
    "    x = x_train[cluster_indices]\n",
    "    if dominant_cluster_class == 0:\n",
    "      l = np.zeros((x.shape[0],))\n",
    "    elif dominant_cluster_class == 1:\n",
    "      l = np.ones((x.shape[0],))\n",
    "    else:\n",
    "        raise ValueError\n",
    "    # save the indices for cluster 7. We'll use these below.\n",
    "    try:\n",
    "      x_labelled = np.concatenate((x_labelled, x))\n",
    "      labels = np.concatenate((labels, l))\n",
    "      labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "    except NameError:\n",
    "      x_labelled = x\n",
    "      labels = l\n",
    "      labelled_indices = cluster_indices\n",
    "\n",
    "m = x_labelled.shape[0]\n",
    "order = np.random.permutation(m)\n",
    "x_labelled = x_labelled[order]\n",
    "x_labelled = np.reshape(x_labelled, (m,20,20), order='F')\n",
    "x_labelled = x_labelled[:,:,:,np.newaxis]\n",
    "labels = labels[order]\n",
    "\n",
    "print(x_labelled.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "unlabelled_indices = np.array([x for x in range(x_train.shape[0]) if x not in labelled_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Assign a more stringent label assignment criteria based on 95% dominant class membership for this data set as there are only 2 classes.  In MNIST cluster contamination was likelt a mixture of a subset of the other 9 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This creates a labelled training set of 1688 subjects, but no cluster with >= 95% class membership has a label of real.  We therefore can't train a machine.  We need to explore the cluster dissolving step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualise some of these clusters\n",
    "\n",
    "Cluster 0 with 113 subjects and 100% bogus subject membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plotCluster(clustering_threepi.labels_, 0, x_train, 20, cmap='hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This cluster could be labelled bogus or 'high signal-to-noise artefacts'.  Or although this cluster contains 100% bogus subjects it might be worth dissolving it further into 2 further classes something like 'masked on the right hand side' and 'saturated source subtraction off centre to the left'.  These classes could be easier for the machine to learn, while an expert could assign a label of bogus to each of these classes without the mamchine having to try and force these into the same output neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualise cluster 3 with 1264 sbjects 56% of which are labelled real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotCluster(clustering_threepi.labels_, 3, x_train, 20, cmap='hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This cluster could be dissolved based on labels of 'real' and  'burntool artefact'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For now, to approximate the process of dissolving a cluster we use the labels of real and bogus to dissolve some of the most confused clusters.  Add a small amount of label noise to replicate volunteer errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def askClusterLabels(cluster_labels, cluster, X, y, image_dims=None, noise_level=None):\n",
    "  cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "  m = cluster_indices.shape[0]\n",
    "  if image_dims:\n",
    "    x = np.reshape(X[cluster_indices], (m, image_dims[0], image_dims[1]), order='F')\n",
    "    x = x[:,:,:,np.newaxis]\n",
    "  else:\n",
    "    x = X[cluster_indices]\n",
    "  cluster_labels = y_train[cluster_indices]\n",
    "  if noise_level:\n",
    "    # add some random noise to these labels \n",
    "    slice = np.random.permutation(m)[:int(noise_level*m)]\n",
    "    cluster_labels[slice] = cluster_labels[slice] != 1\n",
    "  return x, cluster_labels, cluster_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "noise_level=0.1\n",
    "\n",
    "x, cluster_labels, cluster_indices = askClusterLabels(clustering_threepi.labels_, \n",
    "                                                      2,\n",
    "                                                      x_train, \n",
    "                                                      y_train, \n",
    "                                                      (20, 20),\n",
    "                                                      noise_level\n",
    "                                                     )\n",
    "\n",
    "labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "x_labelled = np.concatenate((x_labelled, x))\n",
    "labels = np.concatenate((labels, cluster_labels))\n",
    "\n",
    "x, cluster_labels, cluster_indices = askClusterLabels(clustering_threepi.labels_, \n",
    "                                                      3, \n",
    "                                                      x_train, \n",
    "                                                      y_train, \n",
    "                                                      (20, 20),\n",
    "                                                      noise_level\n",
    "                                                     )\n",
    "\n",
    "labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "x_labelled = np.concatenate((x_labelled, x))\n",
    "labels = np.concatenate((labels, cluster_labels))\n",
    "\n",
    "x, cluster_labels, cluster_indices = askClusterLabels(clustering_threepi.labels_, \n",
    "                                                      7, \n",
    "                                                      x_train, \n",
    "                                                      y_train, \n",
    "                                                      (20, 20),\n",
    "                                                      noise_level\n",
    "                                                     )\n",
    "\n",
    "labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "x_labelled = np.concatenate((x_labelled, x))\n",
    "labels = np.concatenate((labels, cluster_labels))\n",
    "\n",
    "x, cluster_labels, cluster_indices = askClusterLabels(clustering_threepi.labels_, \n",
    "                                                      8, \n",
    "                                                      x_train, \n",
    "                                                      y_train, \n",
    "                                                      (20, 20),\n",
    "                                                      noise_level\n",
    "                                                     )\n",
    "\n",
    "labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "x_labelled = np.concatenate((x_labelled, x))\n",
    "labels = np.concatenate((labels, cluster_labels))\n",
    "\n",
    "x, cluster_labels, cluster_indices = askClusterLabels(clustering_threepi.labels_, \n",
    "                                                      13, \n",
    "                                                      x_train, \n",
    "                                                      y_train, \n",
    "                                                      (20, 20),\n",
    "                                                      noise_level\n",
    "                                                     )\n",
    "\n",
    "labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "x_labelled = np.concatenate((x_labelled, x))\n",
    "labels = np.concatenate((labels, cluster_labels))\n",
    "\n",
    "m = labels.shape[0]\n",
    "order = np.random.permutation(m)\n",
    "\n",
    "unlabelled_indices = np.array([x for x in range(x_train.shape[0]) if x not in labelled_indices])\n",
    "\n",
    "x_labelled = x_labelled[order]\n",
    "labels = labels[order]\n",
    "labels = np_utils.to_categorical(labels, 2)\n",
    "\n",
    "print(x_labelled.shape)\n",
    "print(labels.shape)\n",
    "print(labels)\n",
    "print(np.sum(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now have a data set of 3638 bogus subjects and 2124 real subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Reduce the 400 dimensional pixel space to 3 dimensions and visualise the labelled and unlabelled data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "x_train_pca = pca.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = threeDPlot(x_train_pca, labelled_indices, unlabelled_indices, 'labelled', 'unlabelled')\n",
    "py.iplot(fig, filename='threepi_x_train_pca3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train a generic CNN on the new labelled data set as we did for MNIST.  The architecture is exactly the same here, nothing has been tweaked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model5 = Sequential()\n",
    "\n",
    "model5.add(Conv2D(filters=16, kernel_size=2, padding='valid', \\\n",
    "                   activation='relu', input_shape=(20,20,1)))\n",
    "model5.add(MaxPooling2D(pool_size=2))\n",
    "model5.add(Conv2D(filters=32, kernel_size=2, padding='valid', activation='relu'))\n",
    "model5.add(MaxPooling2D(pool_size=2))\n",
    "model5.add(Conv2D(filters=64, kernel_size=2, padding='valid', activation='relu'))\n",
    "model5.add(MaxPooling2D(pool_size=2))\n",
    "model5.add(GlobalAveragePooling2D('channels_last'))\n",
    "model5.add(Dense(2, activation='softmax'))\n",
    "model5.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fit the model to the data\n",
    "model5.fit(x_labelled, labels, epochs=20, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "m = x_test.shape[0]\n",
    "x_test = np.reshape(x_test, (m, 20, 20), order='F')\n",
    "x_test = x_test[:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# determine if test set class balance\n",
    "print(np.sum(np_utils.to_categorical(y_test, 2), axis=0)/np.sum(np_utils.to_categorical(y_test, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Classes are skewed so accuracy no the best measure.  Calculate the all zeros benchmark as the number to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# deteremine the all zeros benchmark\n",
    "preds = np.zeros(y_test.shape)\n",
    "all_zeros = 100*np.sum(np.argmax(np_utils.to_categorical(preds, 2), axis=1)== \\\n",
    "                    np.argmax(np_utils.to_categorical(y_test, 2), axis=1))/ \\\n",
    "          len(preds)\n",
    "print('All zeros accuracy: %.4f%%' % all_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_accuracy = calculateAccuracy(model5, x_test, y_test, 2)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As with MNIST clone the above network so we can project the data into the learned feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# make a clone of the model above stripping off the output layer and load the weights\n",
    "model6 = Sequential()\n",
    "model6.add(Conv2D(filters=16, kernel_size=2, padding='valid', \\\n",
    "                    activation='relu', input_shape=(20,20,1), \\\n",
    "                    weights=model5.layers[0].get_weights())) # load the learned weights from the previous model\n",
    "model6.add(MaxPooling2D(pool_size=2))\n",
    "model6.add(Conv2D(filters=32, kernel_size=2, padding='valid', \\\n",
    "                    activation='relu',weights=model5.layers[2].get_weights()))\n",
    "model6.add(MaxPooling2D(pool_size=2))\n",
    "model6.add(Conv2D(filters=64, kernel_size=2, padding='valid', \\\n",
    "                    activation='relu',weights=model5.layers[4].get_weights()))\n",
    "model6.add(MaxPooling2D(pool_size=2))\n",
    "model6.add(GlobalAveragePooling2D('channels_last'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "m = x_train.shape[0]\n",
    "x_train = np.reshape(x_train, (m, 20, 20), order='F')\n",
    "x_train = x_train[:,:,:,np.newaxis]\n",
    "activations = model6.predict(x_train) # encode the images\n",
    "print(activations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualise the new data projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "activations_pca = pca.fit_transform(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = threeDPlot(activations_pca, np.where(y_train==1)[0], np.where(y_train==0)[0], 'real', 'bogus')\n",
    "py.iplot(fig, filename='threepi_activations_pca3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Repeat the clustering but in the new feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clustering_activations = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "clustering_activations.fit(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del labels\n",
    "for cluster in range(n_clusters):\n",
    "  cluster_indices = np.where(clustering_activations.labels_ == cluster)[0]\n",
    "  n_assigned_examples = cluster_indices.shape[0]\n",
    "  cluster_labels = one_hot_encoded[cluster_indices]\n",
    "  cluster_label_fractions = np.mean(cluster_labels, axis=0)\n",
    "  dominant_cluster_class = np.argmax(cluster_label_fractions)\n",
    "  print(cluster, n_assigned_examples, dominant_cluster_class, cluster_label_fractions[dominant_cluster_class])\n",
    "  # assign labels based on >= 90% class membership, mimicing human labelling\n",
    "  if cluster_label_fractions[dominant_cluster_class] >= 0.95:\n",
    "    a = activations[cluster_indices]\n",
    "    l = np.zeros((a.shape[0], 2))\n",
    "    l[:,dominant_cluster_class] += 1\n",
    "    try:\n",
    "      a_labelled = np.concatenate((a_labelled, a))\n",
    "      labels = np.concatenate((labels, l))\n",
    "      labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "    except NameError:\n",
    "      a_labelled = a\n",
    "      labels = l\n",
    "      labelled_indices = cluster_indices\n",
    "        \n",
    "print(a_labelled.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "m = a_labelled.shape[0]\n",
    "order = np.random.permutation(m)\n",
    "a_labelled = a_labelled[order]\n",
    "labels = labels[order]\n",
    "\n",
    "unlabelled_indices = np.array([x for x in range(x_train.shape[0]) if x not in labelled_indices])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Again cluster labels assigned based on 95% cluster class membership."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Dissolve the most confused clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "noise_level=0.1\n",
    "\n",
    "x, cluster_labels, cluster_indices = askClusterLabels(clustering_activations.labels_, \n",
    "                                                      3,\n",
    "                                                      activations, \n",
    "                                                      y_train, \n",
    "                                                      noise_level=noise_level\n",
    "                                                     )\n",
    "\n",
    "labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "a_labelled = np.concatenate((a_labelled, x))\n",
    "labels = np.concatenate((labels, np_utils.to_categorical(cluster_labels)))\n",
    "\n",
    "x, cluster_labels, cluster_indices = askClusterLabels(clustering_activations.labels_, \n",
    "                                                      11,\n",
    "                                                      activations, \n",
    "                                                      y_train, \n",
    "                                                      noise_level=noise_level\n",
    "                                                     )\n",
    "\n",
    "labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "a_labelled = np.concatenate((a_labelled, x))\n",
    "labels = np.concatenate((labels, np_utils.to_categorical(cluster_labels)))\n",
    "\n",
    "x, cluster_labels, cluster_indices = askClusterLabels(clustering_activations.labels_, \n",
    "                                                      12,\n",
    "                                                      activations, \n",
    "                                                      y_train, \n",
    "                                                      noise_level=noise_level\n",
    "                                                     )\n",
    "\n",
    "labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "a_labelled = np.concatenate((a_labelled, x))\n",
    "labels = np.concatenate((labels, np_utils.to_categorical(cluster_labels)))\n",
    "\n",
    "x, cluster_labels, cluster_indices = askClusterLabels(clustering_activations.labels_, \n",
    "                                                      16,\n",
    "                                                      activations, \n",
    "                                                      y_train, \n",
    "                                                      noise_level=noise_level\n",
    "                                                     )\n",
    "\n",
    "labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "a_labelled = np.concatenate((a_labelled, x))\n",
    "labels = np.concatenate((labels, np_utils.to_categorical(cluster_labels)))\n",
    "\n",
    "x, cluster_labels, cluster_indices = askClusterLabels(clustering_activations.labels_, \n",
    "                                                      17,\n",
    "                                                      activations, \n",
    "                                                      y_train, \n",
    "                                                      noise_level=noise_level\n",
    "                                                     )\n",
    "\n",
    "labelled_indices = np.concatenate((labelled_indices, cluster_indices))\n",
    "a_labelled = np.concatenate((a_labelled, x))\n",
    "labels = np.concatenate((labels, np_utils.to_categorical(cluster_labels)))\n",
    "print(a_labelled.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This produces a labelled data set with 9179 subjects with classes skewed as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(np.sum(labels, axis=0)/np.sum(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Build a fully connected Neural Net with a single hidden layer to learn these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model7 = Sequential()\n",
    "model7.add(Dense(500, activation='relu', input_shape=(activations.shape[1],)))\n",
    "#model.add(Dropout(0.3))\n",
    "model7.add(Dense(2, activation='softmax'))\n",
    "model7.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fit the model to the data\n",
    "model7.fit(a_labelled, labels, epochs=20, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_activations = model6.predict(x_test)\n",
    "test_accuracy = calculateAccuracy(model7, test_activations, y_test, 2)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is an improvement on the 91.4% we had above.  This is a small improvement and the weakness might be assigning subjects to only one of 2 classes."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
