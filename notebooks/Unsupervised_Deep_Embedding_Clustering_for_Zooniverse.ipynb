{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Deep Embedding Clustering for Zooniverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the approach from [Unsupervised Deep Embedding for Clustering Analysis](https://arxiv.org/abs/1511.06335). I'm using the keras implementation [here](https://github.com/XifengGuo/DEC-keras).\n",
    "\n",
    "The algorithm uses greedy layer-wise pretaining of a [deep denoising autoencoder](https://blog.keras.io/building-autoencoders-in-keras.html) to learn an initial embedding that minimises reconstruction loss.  Clusters are then initialised using K-means.  Training examples (subjects) are then encoded by the auto encoder and assigned to clusters by a soft assignment based on proximity to a cluster centre.  The network parameters and cluster centres are then further trained by minimising the KL divergence between the soft cluster assignments and an auxiliary distribution.  The auxiliary distribution assumes that those examples lying closest to a cluster centre have a high confidence of belonging to the same class (or at least share some relationship that is worth reinforcing).  The auxiliary distribution also normalises the loss contribution of each centroid such that the loss is not completely dominated by the largest centroids.\n",
    "\n",
    "Applying this to Zooniverse.\n",
    "\n",
    "#### Experiment 1\n",
    "Run the MNIST experiment from the original paper.  This is our completely unsupervised benchmark.  The clusters currently have no meaning.\n",
    "\n",
    "The original paper uses the entire 70000 MNIST data set.  We should experiment with how varying the size of the initial training affects performance.  For the rest of our experiments we will need a test set so we should at least hold out the 10000 MNIST test set reducing the training set size to 60000 subjects.\n",
    "\n",
    "#### Experiment 2\n",
    "Simulate querying cluster labels from volunteers for the clustering in experiment 1. Tack a layer that maps each cluster to a label onto the network created in experiment 1.  Then train the network with the MNIST labels.  This replicates querying labels from perfect classifiers for every subject in the MNIST data set.  How does this performance compare to the unsupervised benchmark.  How does this compare to directly training a network from scratch on the labels.  We should observe a shorter time to convergence and potentially better performance.  Experiment with differing levels of volunteer classification noise and which subjects we should query labels for.\n",
    "\n",
    "#### Experiment 3\n",
    "Repeat experiment 1 and 2 for Supernova Hunters data.\n",
    "\n",
    "#### Experiment 4\n",
    "Repeat experiment 1 and 2 for CIFAR-10, CIFAR-100 and STL-10 datasets.  These are more similar to ecology projects.  STL-10 in particular might be interesting as there are only 500 training images and 800 test images  per class but has 100000 unlabelled images.  Might be interesting to test serendipitous discovery and the presence of uninteresting classes.  We could also test transfer learning between data sets.\n",
    "\n",
    "#### Experiment 5\n",
    "Repeat experiment 1 and 2 but with Marcos Ecology Project data set.\n",
    "\n",
    "#### Experiment 6\n",
    "Use Marcos pretrained CNN to replace the deep autoencoder in experiment 1.  This investigates transfer learning applied to our clustering approach to gathering labels.\n",
    "\n",
    "#### Experiment 7\n",
    "How can the idea of dissolving clusters be incorporated into the network architecture.  Should we just 'delete' that cluster in which case all its members will be assigned to their next best cluster? Should we just randomly reinitialise the cluster (this runs the risk of undoing earlier training)? Should we divide the cluster into 2 new clusters with the new cluster centres initialised based on some information we have about where different classes lie within the original cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the DEC implementation and various helper functions from the DEC-keras repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0,'../DEC-keras/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from DEC import DEC, cluster_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the mnist data set normalised as in the DEC paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST samples (70000, 784)\n"
     ]
    }
   ],
   "source": [
    "x, y = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some contants from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters = 10 # this is chosen based on prior knowledge of classes in the data set.\n",
    "batch_size = 256\n",
    "lr         = 0.01 # learning rate\n",
    "momentum   = 0.9\n",
    "tol        = 0.001 # tolerance - if clustering stops if less than this fraction of the data changes cluster on an interation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some constants for this implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxiter         = 2e4\n",
    "update_interval = 140\n",
    "save_dir         = '../DEC-keras/results/dec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare the DEC model\n",
    "dec = DEC(dims=[x.shape[-1], 500, 500, 2000, 10], n_clusters=n_clusters, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have already run the greedy layer-wise pretraining so tell DEC where to find those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae_weights = '../DEC-keras/ae_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dec.initialize_model(optimizer=SGD(lr=lr, momentum=momentum),\n",
    "                     ae_weights=ae_weights,\n",
    "                     x=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a summary of the model archietecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 10)                100       \n",
      "=================================================================\n",
      "Total params: 1,665,110.0\n",
      "Trainable params: 1,665,110.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 140\n",
      "Save interval 1367.1875\n",
      "Initializing cluster centers with k-means.\n",
      "Iter 0 : Acc 0.87557 , nmi 0.8581 , ari 0.82627 ; loss= 0\n",
      "saving model to: ../DEC-keras/results/dec//DEC_model_0.h5\n",
      "Iter 140 : Acc 0.87553 , nmi 0.85805 , ari 0.82621 ; loss= 0.03464\n",
      "delta_label  0.000542857142857 < tol  0.001\n",
      "Reached tolerance threshold. Stopping training.\n",
      "saving model to: ../DEC-keras/results/dec//DEC_model_final.h5\n",
      "acc: 0.875528571429\n",
      "clustering time:  69.14353013038635\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "y_pred = dec.clustering(x, y=y, tol=tol, maxiter=maxiter,\n",
    "                        update_interval=update_interval, save_dir=save_dir)\n",
    "print('acc:', cluster_acc(y, y_pred))\n",
    "print('clustering time: ', (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above gives our Unsupervised benchmark accuracy of 87.55% on the entire MNIST data set (70000 subjects)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
