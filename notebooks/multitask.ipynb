{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/dwright/anaconda2/envs/aind/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/dwright/anaconda2/envs/aind/lib/python3.6/site-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_curve\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.initializers import Initializer\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "\n",
    "sys.path.insert(0,'../DEC-keras')\n",
    "from DEC import DEC\n",
    "\n",
    "sys.path.insert(0,'../experiments/dissolving')\n",
    "from dissolving_utils import load_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_to_label_mapping_safe(y, y_pred, n_classes, n_clusters):\n",
    "  \"\"\"Enusre at least one cluster assigned to each label.\n",
    "  \"\"\"\n",
    "  one_hot_encoded = np_utils.to_categorical(y, n_classes)\n",
    "\n",
    "  cluster_to_label_mapping = []\n",
    "  n_assigned_list = []\n",
    "  majority_class_fractions = []\n",
    "  majority_class_pred = np.zeros(y.shape)\n",
    "  for cluster in range(n_clusters):\n",
    "    cluster_indices = np.where(y_pred == cluster)[0]\n",
    "    n_assigned_examples = cluster_indices.shape[0]\n",
    "    n_assigned_list.append(n_assigned_examples)\n",
    "    cluster_labels = one_hot_encoded[cluster_indices]\n",
    "    cluster_label_fractions = np.mean(cluster_labels, axis=0)\n",
    "    majority_cluster_class = np.argmax(cluster_label_fractions)\n",
    "    cluster_to_label_mapping.append(majority_cluster_class)\n",
    "    majority_class_pred[cluster_indices] += majority_cluster_class\n",
    "    majority_class_fractions.append(cluster_label_fractions[majority_cluster_class])\n",
    "    print(cluster, n_assigned_examples, majority_cluster_class, cluster_label_fractions[majority_cluster_class])\n",
    "  #print(cluster_to_label_mapping)\n",
    "\n",
    "  print(np.unique(y), np.unique(cluster_to_label_mapping))\n",
    "  try:\n",
    "    # make sure there is at least 1 cluster representing each class\n",
    "    assert np.all(np.unique(y) == np.unique(cluster_to_label_mapping))\n",
    "  except AssertionError:\n",
    "    # if there is no cluster for a class then we will assign a cluster to that\n",
    "    # class\n",
    "    \n",
    "    # find which class it is\n",
    "    # ASSUMPTION - this task is binary\n",
    "    \n",
    "    diff = list(set(np.unique(y)) - set(np.unique(cluster_to_label_mapping)))[0]\n",
    "      # we choose the cluster that contains the most examples of the class with no cluster\n",
    "      \n",
    "    one_hot = np_utils.to_categorical(y_pred[np.where(y==diff)[0]], \\\n",
    "                                        len(cluster_to_label_mapping))\n",
    "                                      \n",
    "    cluster_to_label_mapping[np.argmax(np.sum(one_hot, axis=0))] = int(diff)\n",
    "  print(cluster_to_label_mapping)\n",
    "  return cluster_to_label_mapping, n_assigned_list, majority_class_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dec(x, ae_weights, dec_weights, n_clusters, batch_size, lr, momentum):\n",
    "  dec = DEC(dims=[x.shape[-1], 500, 500, 2000, 10], n_clusters=n_clusters, batch_size=batch_size)\n",
    "  ae_weights = ae_weights\n",
    "  dec.initialize_model(optimizer=SGD(lr=lr, momentum=momentum),\n",
    "                       ae_weights=ae_weights,\n",
    "                       x=x, loss='kld')\n",
    "  dec.load_weights(dec_weights)\n",
    "  dec.model.summary()\n",
    "  return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percent_fpr(y, pred, fom):\n",
    "  fpr, tpr, thresholds = roc_curve(y, pred)\n",
    "  FoM = 1-tpr[np.where(fpr<=fom)[0][-1]] # MDR at 1% FPR\n",
    "  threshold = thresholds[np.where(fpr<=fom)[0][-1]]\n",
    "  return FoM, threshold, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MapInitializer(Initializer):\n",
    "    \n",
    "  def __init__(self, mapping, n_classes):\n",
    "    self.mapping = mapping\n",
    "    self.n_classes = n_classes\n",
    "\n",
    "  def __call__(self, shape, dtype=None):\n",
    "    return K.one_hot(self.mapping, self.n_classes)\n",
    "    #return K.ones(shape=(100,10))\n",
    "\n",
    "  def get_config(self):\n",
    "    return {'mapping': self.mapping, 'n_classes': self.n_classes}\n",
    "\n",
    "class MappingLayer(Layer):\n",
    "\n",
    "  def __init__(self, mapping, output_dim, kernel_initializer, **kwargs):\n",
    "  #def __init__(self, mapping, output_dim, **kwargs):\n",
    "    self.output_dim = output_dim\n",
    "    # mapping is a list where the index corresponds to a cluster and the value is the label.\n",
    "    # e.g. say mapping[0] = 5, then a label of 5 has been assigned to cluster 0\n",
    "    self.n_classes = np.unique(mapping).shape[0]      # get the number of classes\n",
    "    self.mapping = K.variable(mapping, dtype='int32')\n",
    "    self.kernel_initializer = kernel_initializer\n",
    "    super(MappingLayer, self).__init__(**kwargs)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "  \n",
    "    self.kernel = self.add_weight(name='kernel', \n",
    "                                  shape=(input_shape[1], self.output_dim),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  trainable=False)\n",
    "  \n",
    "    super(MappingLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "  def call(self, x):\n",
    "    return K.softmax(K.dot(x, self.kernel))\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultitaskDEC(DEC):\n",
    "  def clustering(self, x, y=None, validation_data=None, tol=1e-3, update_interval=140, maxiter=2e4, save_dir='./results/dec', pretrained_weights=None):\n",
    "    print('Update interval', update_interval)\n",
    "    save_interval = x.shape[0] / self.batch_size * 5  # 5 epochs\n",
    "    print('Save interval', save_interval)\n",
    "\n",
    "    try:\n",
    "      self.load_weights(pretrained_weights)\n",
    "    except AttributeError:\n",
    "      # initialize cluster centers using k-means\n",
    "      print('Initializing cluster centers with k-means.')\n",
    "      kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "      y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "      y_pred_last = y_pred\n",
    "      self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "    y_p = self.predict_clusters(x)\n",
    "    self.n_classes = y.shape[1]\n",
    "    \n",
    "    # ensure at least one cluster assigned to each class\n",
    "    cluster_to_label_mapping, n_assigned_list, majority_class_fractions = \\\n",
    "      get_cluster_to_label_mapping_safe(y[:,1], y_p, self.n_classes, self.n_clusters)\n",
    "    \n",
    "    # hack - ensure the cluster with the most real examples is assigned to the real class.\n",
    "    #print(np.argmax((1-np.array(majority_class_fractions))*np.array(n_assigned_list)))\n",
    "    #cluster_to_label_mapping[np.argmax((1-np.array(majority_class_fractions))*np.array(n_assigned_list))] = 1\n",
    "    \n",
    "    a = Input(shape=(x.shape[1],)) # input layer\n",
    "    q_out = self.model(a)\n",
    "    pred = MappingLayer(cluster_to_label_mapping, output_dim=self.n_classes, \\\n",
    "      name='mapping', kernel_initializer=MapInitializer(cluster_to_label_mapping, self.n_classes))(q_out)\n",
    "    self.model = Model(inputs=a, outputs=[pred, q_out])\n",
    "\n",
    "    optimizer = 'adam'\n",
    "    self.model.compile(optimizer=optimizer, loss={'mapping': 'categorical_crossentropy', 'model_3': 'kld'}, \\\n",
    "                                      loss_weights={'mapping': 1, 'model_3': 1})\n",
    "\n",
    "    loss = [np.inf, np.inf, np.inf]\n",
    "    index = 0\n",
    "    q = self.model.predict(x, verbose=0)[1]\n",
    "    y_pred_last = q.argmax(1)\n",
    "    best_val_loss = [np.inf, np.inf, np.inf]\n",
    "    for ite in range(int(maxiter)):\n",
    "      if ite % update_interval == 0:\n",
    "        q = self.model.predict(x, verbose=0)[1]\n",
    "        p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "        # evaluate the clustering performance\n",
    "        y_pred = q.argmax(1)\n",
    "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "        y_pred_last = y_pred\n",
    "        y_pred = self.model.predict(x)[0]\n",
    "        if y is not None:\n",
    "          loss = np.round(loss, 5)\n",
    "          valid_p = self.target_distribution(self.model.predict(validation_data[0], verbose=0)[1])\n",
    "          val_loss = np.round(self.model.test_on_batch(validation_data[0], [validation_data[1], valid_p]), 5)\n",
    "          f, _, _, _ = percent_fpr(y[:,1], y_pred[:,1], 0.1)\n",
    "          f = np.round(f, 5)\n",
    "          f1 = np.round(f1_score(y[:,1], np.argmax(y_pred, axis=1)), 5)\n",
    "          y_pred_valid = self.model.predict(validation_data[0])[0]\n",
    "          f_valid, _, _, _ = percent_fpr(validation_data[1][:,1], y_pred_valid[:,1], 0.1)\n",
    "          f_valid = np.round(f_valid, 5)\n",
    "          f1_valid = np.round(f1_score(validation_data[1][:,1], np.argmax(y_pred_valid, axis=1)), 5)\n",
    "          print('Iter', ite, ' :MDR at 10% FPR', f, ', F1=', f1, '; loss=', loss, \\\n",
    "                '; valid_loss=,', val_loss, '; valid MDR at 10% FPR=,', f_valid, ', valid F1=', f1_valid)\n",
    "          if val_loss[1] < best_val_loss[1]: # only interested in classification improvements\n",
    "            print('saving model: ', best_val_loss, ' -> ', val_loss)\n",
    "            self.model.save_weights('best_val_loss.hf')\n",
    "            best_val_loss = val_loss\n",
    "      \n",
    "        # train on batch\n",
    "        if (index + 1) * self.batch_size > x.shape[0]:\n",
    "          loss = self.model.train_on_batch(x=x[index * self.batch_size::],\n",
    "                                           y=[y[index * self.batch_size::], \\\n",
    "                                              p[index * self.batch_size::]])\n",
    "          index = 0\n",
    "        else:\n",
    "          loss = self.model.train_on_batch(x=x[index * self.batch_size:(index + 1) * self.batch_size],\n",
    "                                           y=[y[index * self.batch_size:(index + 1) * self.batch_size], \\\n",
    "                                              p[index * self.batch_size:(index + 1) * self.batch_size]])\n",
    "          index += 1\n",
    "\n",
    "        # save intermediate model\n",
    "        if ite % save_interval == 0:\n",
    "        # save IDEC model checkpoints\n",
    "          print('saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "          self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "\n",
    "        ite += 1\n",
    "\n",
    "    # save the trained model\n",
    "    print('saving model to:', save_dir + '/DEC_model_final.h5')\n",
    "    self.model.save_weights(save_dir + '/DEC_model_final.h5')\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "  def predict_clusters(self, x):  # predict cluster labels using the output of clustering layer\n",
    "    q = self.model.predict(x, verbose=0)\n",
    "    try:\n",
    "      return q.argmax(1)\n",
    "    except AttributeError:\n",
    "      return q[1].argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in some volunteer labelled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,31):\n",
    "  data = sio.loadmat('../data/snhunters/3pi_20x20_supernova_hunters_batch_%d_signPreserveNorm_detect_misaligned.mat'%(i))\n",
    "  try:\n",
    "    x_train = np.concatenate((x_train, np.nan_to_num(np.reshape(data['X'], \\\n",
    "      (data['X'].shape[0], 400), order='F'))))\n",
    "    y_train = np.concatenate((y_train, np.squeeze(data['y'])))\n",
    "  except NameError:\n",
    "    x_train = np.nan_to_num(np.reshape(data['X'], (data['X'].shape[0], 400), \\\n",
    "      order='F'))\n",
    "    y_train = np.squeeze(data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only use the first classifications of subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u, indices = np.unique(x_train, return_index=True, axis=0)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# divide into training and validation sets\n",
    "m = x_train.shape[0]\n",
    "split = int(.75*m)\n",
    "\n",
    "x_valid = x_train[split:]\n",
    "y_valid = y_train[split:]\n",
    "\n",
    "x_train = x_train[:split]\n",
    "y_train = y_train[:split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the multitask DEC initiaising it with the previously trainined weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae_weights = '../DEC-keras/results/snh/ae_weights_snh.h5' # previously trained Auto-encoder weights\n",
    "dec_weights = '../DEC-keras/results/snh/10/DEC_model_final.h5' # previously trained DEC weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dec = MultitaskDEC(dims=[x_train.shape[-1], 500, 500, 2000, 10], \\\n",
    "                   n_clusters=10, batch_size=256)\n",
    "dec.initialize_model(optimizer=SGD(lr=0.01, momentum=0.9),\n",
    "                     ae_weights=ae_weights,\n",
    "                     x=x_train)\n",
    "dec.model.load_weights(dec_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the DEC model using the volunteer labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only running for 1000 iterations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 140\n",
      "Save interval 454.21875\n",
      "0 140 0 0.75\n",
      "1 965 0 0.6093264248704663\n",
      "2 1395 0 0.589247311827957\n",
      "3 6120 0 0.6168300653594772\n",
      "4 2072 0 0.6013513513513513\n",
      "5 2610 0 0.542911877394636\n",
      "6 4231 0 0.6343653982510045\n",
      "7 4571 0 0.6226208707066287\n",
      "8 4 1 0.75\n",
      "9 1148 0 0.578397212543554\n",
      "[0. 1.] [0 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dwright/anaconda2/envs/aind/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0  :MDR at 10% FPR 0.90217 , F1= 0.0 ; loss= [inf inf inf] ; valid_loss=, [0.77632 0.67048 0.10584] ; valid MDR at 10% FPR=, 0.91792 , valid F1= 0.0\n",
      "saving model:  [inf, inf, inf]  ->  [0.77632 0.67048 0.10584]\n",
      "saving model to: ./results/dec/DEC_model_0.h5\n",
      "Iter 140  :MDR at 10% FPR 0.90086 , F1= 0.0 ; loss= [0.77405 0.6738  0.10025] ; valid_loss=, [0.87027 0.66555 0.20472] ; valid MDR at 10% FPR=, 0.92652 , valid F1= 0.0\n",
      "saving model:  [0.77632 0.67048 0.10584]  ->  [0.87027 0.66555 0.20472]\n",
      "Iter 280  :MDR at 10% FPR 0.89756 , F1= 0.0 ; loss= [0.81072 0.60038 0.21034] ; valid_loss=, [0.8455  0.66807 0.17743] ; valid MDR at 10% FPR=, 0.92222 , valid F1= 0.0\n",
      "Iter 420  :MDR at 10% FPR 0.90821 , F1= 0.0 ; loss= [0.836   0.66175 0.17425] ; valid_loss=, [0.8486  0.66799 0.1806 ] ; valid MDR at 10% FPR=, 0.92079 , valid F1= 0.0\n",
      "Iter 560  :MDR at 10% FPR 0.90075 , F1= 0.0 ; loss= [0.86913 0.69609 0.17304] ; valid_loss=, [0.83977 0.66841 0.17136] ; valid MDR at 10% FPR=, 0.92043 , valid F1= 0.0\n",
      "Iter 700  :MDR at 10% FPR 0.90108 , F1= 0.0 ; loss= [0.80208 0.63494 0.16713] ; valid_loss=, [0.8154  0.66928 0.14612] ; valid MDR at 10% FPR=, 0.91111 , valid F1= 0.0\n",
      "Iter 840  :MDR at 10% FPR 0.90766 , F1= 0.0 ; loss= [0.83359 0.68256 0.15103] ; valid_loss=, [0.8281  0.66872 0.15937] ; valid MDR at 10% FPR=, 0.91756 , valid F1= 0.0\n",
      "Iter 980  :MDR at 10% FPR 0.90327 , F1= 0.0 ; loss= [0.84442 0.69051 0.15391] ; valid_loss=, [0.826   0.66877 0.15723] ; valid MDR at 10% FPR=, 0.91398 , valid F1= 0.0\n",
      "saving model to: ./results/dec/DEC_model_final.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.72151625, 0.27848372],\n",
       "       [0.72716945, 0.2728305 ],\n",
       "       [0.7108438 , 0.28915617],\n",
       "       ...,\n",
       "       [0.7234734 , 0.2765265 ],\n",
       "       [0.722374  , 0.277626  ],\n",
       "       [0.72217363, 0.27782634]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.clustering(x_train, np_utils.to_categorical(y_train), \\\n",
    "              (x_valid, np_utils.to_categorical(y_valid)), \\\n",
    "               pretrained_weights=dec_weights, maxiter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [aind]",
   "language": "python",
   "name": "Python [aind]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
